cor.names <- colnames(cor.mat.ord)
final.names <- cor.names[name.index]
# pull out the original data from the above variables
og.index <- match(final.names, colnames(new))
out <- new[,og.index]
return(data.frame(out))
}
# set the R2 cutoff point for the above function
R <- 0.5
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
head(reduced.all.data)
colnames(reduced.all.data)
all.npp <- reduced.all.data[,1]
all.topo <- reduced.all.data[,c(6,14:17)]
all.structure <- reduced.all.data[,c(5,7,8,10,11:13)]
all.geography <- reduced.all.data[,c(2,49)]
all.sites <- reduced.all.data[,3]
mod <- varpart(all.npp, all.topo, all.structure, all.geography, all.sites, transfo = "hel")
all.npp <- reduced.all.data[,1]
all.topo <- reduced.all.data[,c(6,14:17)]
all.structure <- reduced.all.data[,c(5,7,8,10,11:13)]
all.geography <- reduced.all.data[,c(2,9)]
all.sites <- reduced.all.data[,3]
mod <- varpart(all.npp, all.topo, all.structure, all.geography, all.sites, transfo = "hel")
?varpart
mod <- varpart(all.npp, all.topo, all.structure, all.geography, all.sites)
mod
mod <- varpart(mite, ~ ., mite.pcnm, data=mite.env, transfo="hel")
mod
mod <- varpart(all.npp, all.topo, all.structure, all.geography, all.sites)
mod
mod <- varpart(all.npp, ~., all.topo, all.structure, all.geography, all.sites)
# load packages
library(MASS)		     # stepAIC
library(ape)		     # Moran's I
library(fields)		   # euclidean distance
library(spdep)	 	   # SAR
library(ncf)		     # correllograms
library(raster)		   # rasters
library(spatialreg)  # SAR
# read in the data
in.data <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications",
"/Kamoske_Dissteration_Ch3/data/sar_modeling/20200220/data/all_sites_data.csv"))
# find the number of rows in the data for later usage
n <- dim(in.data)[1]
# convert to a raster to see what we are looking at
for.map <- rasterFromXYZ(as.matrix(in.data[,1:3]))
# read in the data
in.data <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications",
"/Kamoske_Dissteration_Ch3/data/sar_modeling/20200220/data/all_sites_data.csv"))
# find the number of rows in the data for later usage
n <- dim(in.data)[1]
# # convert to a raster to see what we are looking at
# for.map <- rasterFromXYZ(as.matrix(in.data[,1:3]))
# plot(for.map)
# ONLY DO THIS IF WE NEED TO TAKE A SUBSET
# subset data taking every 10th point
# note: probably unrealistic to run SAR on >10000 points depending on computing power
to.keep <- as.logical(rep(c(1,rep(0,4)), length.out = n))
in.data.sub <- subset(in.data, to.keep)
# IF WE DON'T NEED A SUBSET THEN WE CAN DO THIS
# in.data.sub <- in.data
# # plot the sub data
# for.map.sub <- rasterFromXYZ(as.matrix(in.data.sub[,1:3]))
# plot(for.map.sub)
# store the true x & y locations (eastings & northings) as a matrix
locs.short <- as.matrix(in.data.sub[,1:2])
plot(locs.short)
# this function makes all variables mean zero, sd 0.5
# Gelman 2008 "Scaling regression inputs by dividing by two standard deviations"
standard <- function(in.column){
(in.column - mean(in.column))/ (2* sd(in.column))
}
# so you don't want to scale binary data, just continuous
in.data.to.scale.short <- in.data.sub[,3:43]
scaled.in.data.short <- as.data.frame(apply(in.data.to.scale.short, 2, standard))
# calculate geographical trend variables using the scaled data
x.sc <- standard(locs.short[,1])
y.sc <- standard(locs.short[,2])
x.sq <- x.sc^2
x.y <- x.sc * y.sc
y.sq <- y.sc^2
x.cub <- x.sc^3
y.cub <- y.sc^3
x2.y <- x.sq * y.sc
x.y2 <- x.sc * y.sq
geographic <- cbind(x.sc, y.sc, x.sq, x.y, y.sq, x.cub, y.cub, x2.y, x.y2)
# now we scale those geographic trends again
geog.sc <- as.data.frame(apply(geographic, 2, standard))
# recombine dataset, with the scaled original data, the scaled geographic data, and the
# original binary variables
all.data <- cbind(scaled.in.data.short, geog.sc, in.data.sub[,44:48])
head(all.data)
#-----------------------------------------------------------------------------------------------------#
# remove multicollinear data
#-----------------------------------------------------------------------------------------------------#
# write up a function to use to remove correlated data
rem.cors <- function(frame) {
# find the number of columns and rows in the dataframe
b <- ncol(frame)
n <- nrow(frame)
# create an empty dataframe
keep1 <- array(0, dim = c(1, b))
# remove columns that are either all zeros or all the same value
for (h in 1:b) {
ifelse(sum(frame[,h]) != 0, keep1[,h] <- h, keep1[,h] <- 0)
ifelse(sum(duplicated(frame[,h])) != n - 1 ,keep1[,h] <- h, keep1[,h] <- 0)
}
# only keep the columns that have values
g <- as.numeric(keep1[keep1 > 0])
new <- frame[,g]
# find the number of columns left after removing no data values
c <- ncol(new)
# create a correlation matrix (will be c x c dimensions)
cor.mat <- cor(new, method = "pearson", use = "na.or.complete")
# make an empty matrix filled with zeros
keep <- array(0, dim = c(1,c))
# make an empty matrix filled with ones
m <- matrix(1, nrow = n, ncol = c)
# now we need to reorder the columns so that they are in the order of most correlated
cor.mat.ord <- cor.mat[,order(-abs(cor.mat[1,]))]
cor.mat.ord <- cor.mat.ord[order(-abs(cor.mat[,1])),]
# loop through the data and remove the data that is too correlated to one another
for (i in 2:c) {
if (i == 2) {
m[,i] <- m[,i]
}
if (i > 2) {
red.mat <- m[,1:(i-1)]
cor.index <- which(red.mat[1,] == 1)
var.cors <- as.numeric(abs(cor.mat.ord[i, cor.index[2:length(cor.index)]]))
ifelse(any(var.cors > R), m[,i] <- 0, m[,i] <- m[,i])
}
}
# save the column names of the variables that we kept
name.index <- which(m[1,] == 1)
cor.names <- colnames(cor.mat.ord)
final.names <- cor.names[name.index]
# pull out the original data from the above variables
og.index <- match(final.names, colnames(new))
out <- new[,og.index]
return(data.frame(out))
}
# set the R2 cutoff point for the above function
# try raising this to make sure we are avoiding multicollinearity
R <- 0.5
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
# set the R2 cutoff point for the above function
# try raising this to make sure we are avoiding multicollinearity
R <- 0.8
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
# set the R2 cutoff point for the above function
# try changing this to make sure we are avoiding multicollinearity
R <- 0.3
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
# run an OLS with the above predictors
npp.model <- lm(npp ~ .,
data = reduced.all.data)
1.5*60
90/24
13*5
65/24
# if you're running this for the first time on this computer, use the following:
install.packages("randomForest")
# now we have to load the package:
library(randomForest)
63*3
189/24
63*2.5/24
63*2/24
library(hypRspec)
# list the files
hsi.files <- list.files("D:/corrected_hsi_hdf5_files/mlbs2018",
full.names = TRUE)
hsi.files
i <- 1
strsplit(hsi.files[i], "_")[[1]][6]
# list the files
hsi.files <- list.files("D:/corrected_hsi_hdf5_files/mlbs2018",
full.names = TRUE)
hsi.files
# loop through the files and extract the random points
for (i in 1:length(hsi.files)) {
# extract the data for the flight line
hsi.refl <- hsi.random.extract(hy.file = hsi.files[1],
metadata.path = "/MLBS/Reflectance/Reflectance_Data",
coordinate.path = "/MLBS/Reflectance/Metadata/Coordinate_System",
wavelength.path = "/MLBS/Reflectance/Metadata/Spectral_Data/Wavelength",
reflectance.path = "/MLBS/Reflectance/Reflectance_Data",
band.combo = c(25:194, 215:284, 325:403),
number.pts = 58)
# pull out the file name
f.name <- strsplit(hsi.files[i], "_")[[1]][6]
# write the data to disc
write.csv(hsi.refl,
paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/mlbs/flightlines/refl_", f.name, ".csv"),
row.names = FALSE)
}
# list all the individual flightlines
f.lines <- list.files(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/mlbs/flightlines"),
full.names = TRUE)
# read in all the files and combine then
refl.data <- do.call(rbind, lapply(f.lines, read.csv, skip = 1))
# remove the data we don't need
refl.data$ID <- "mlbs"
refl.data$Fline <- NULL
# write this combined file to disc
write.csv(refl.data,
paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/mlbs/mlbs2018_reflectance.csv"),
row.names = FALSE)
library(hypRspec)
# list the files
hsi.files <- list.files("D:/corrected_hsi_hdf5_files/harv2018",
full.names = TRUE)
#---------------------------------------------------------------#
# we will extract 2000 random samples per site
# TALL = 74 samples per flight line
# SERC = 76 samples per flight line
# MLBS = 58 samples per flight line
# HARV = 50 samples per flight line
# ORNL = 31 samples per flight line
#---------------------------------------------------------------#
# loop through the files and extract the random points
for (i in 1:length(hsi.files)) {
# extract the data for the flight line
hsi.refl <- hsi.random.extract(hy.file = hsi.files[1],
metadata.path = "/HARV/Reflectance/Reflectance_Data",
coordinate.path = "/HARV/Reflectance/Metadata/Coordinate_System",
wavelength.path = "/HARV/Reflectance/Metadata/Spectral_Data/Wavelength",
reflectance.path = "/HARV/Reflectance/Reflectance_Data",
band.combo = c(25:194, 215:284, 325:403),
number.pts = 50)
# pull out the file name
f.name <- strsplit(hsi.files[i], "_")[[1]][6]
# write the data to disc
write.csv(hsi.refl,
paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/harv/flightlines/refl_", f.name, ".csv"),
row.names = FALSE)
}
# list all the individual flightlines
f.lines <- list.files(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/harv/flightlines"),
full.names = TRUE)
# read in all the files and combine then
refl.data <- do.call(rbind, lapply(f.lines, read.csv, skip = 1))
# remove the data we don't need
refl.data$ID <- "harv"
refl.data$Fline <- NULL
# write this combined file to disc
write.csv(refl.data,
paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/harv/harv2018_reflectance.csv"),
row.names = FALSE)
63/8
# once all the files are ran we can combine them into one file
tall <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/tall/tall2018_reflectance.csv"))
serc <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/serc/serc2017_reflectance.csv"))
harv <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/harv/harv2018_reflectance.csv"))
mlbs <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/mlbs/mlbs2018_reflectance.csv"))
ornl
# make sure that the colnames match
colnames(serc) <- colnames(tall)
colnames(mlbs) <- colnames(tall)
colnames(harv) <- colnames(tall)
# combine the data
all.data <- rbind(tall,serc,mlbs,harv)
# write this file to disc
write.csv(all.data,
paste0("C:/Users/Aaron Kamoske/Dropbox/Publications/Kamoske_Dissteration_Ch3/",
"data/spectral_diversity/reflectance_extract/all_sites_reflectance.csv"),
row.names = FALSE)
colnames(all.data)
dim(all.data)
spec.data <- all.data[,2:320]
# lets run a pca on the training data
site.pca <- prcomp(spec.data,
center = TRUE,
scale = FALSE)
summary(site.pca)
42/8
# load packages
library(MASS)		     # stepAIC
library(ape)		     # Moran's I
library(fields)		   # euclidean distance
library(spdep)	 	   # SAR
library(ncf)		     # correllograms
library(raster)		   # rasters
library(spatialreg)  # SAR
# read in the data
in.data <- read.csv(paste0("C:/Users/Aaron Kamoske/Dropbox/Publications",
"/Kamoske_Dissteration_Ch3/data/sar_modeling/20200220/data/all_sites_data.csv"))
# find the number of rows in the data for later usage
n <- dim(in.data)[1]
# subset data taking every 10th point
# note: probably unrealistic to run SAR on >10000 points depending on computing power
to.keep <- as.logical(rep(c(1,rep(0,4)), length.out = n))
in.data.sub <- subset(in.data, to.keep)
# store the true x & y locations (eastings & northings) as a matrix
locs.short <- as.matrix(in.data.sub[,1:2])
plot(locs.short)
# this function makes all variables mean zero, sd 0.5
# Gelman 2008 "Scaling regression inputs by dividing by two standard deviations"
standard <- function(in.column){
(in.column - mean(in.column))/ (2* sd(in.column))
}
# so you don't want to scale binary data, just continuous
in.data.to.scale.short <- in.data.sub[,3:43]
scaled.in.data.short <- as.data.frame(apply(in.data.to.scale.short, 2, standard))
# recombine dataset, with the scaled original data, the scaled geographic data, and the
# original binary variables
all.data <- cbind(scaled.in.data.short, in.data.sub[,44:48])
head(all.data)
# write up a function to use to remove correlated data
rem.cors <- function(frame) {
# find the number of columns and rows in the dataframe
b <- ncol(frame)
n <- nrow(frame)
# create an empty dataframe
keep1 <- array(0, dim = c(1, b))
# remove columns that are either all zeros or all the same value
for (h in 1:b) {
ifelse(sum(frame[,h]) != 0, keep1[,h] <- h, keep1[,h] <- 0)
ifelse(sum(duplicated(frame[,h])) != n - 1 ,keep1[,h] <- h, keep1[,h] <- 0)
}
# only keep the columns that have values
g <- as.numeric(keep1[keep1 > 0])
new <- frame[,g]
# find the number of columns left after removing no data values
c <- ncol(new)
# create a correlation matrix (will be c x c dimensions)
cor.mat <- cor(new, method = "pearson", use = "na.or.complete")
# make an empty matrix filled with zeros
keep <- array(0, dim = c(1,c))
# make an empty matrix filled with ones
m <- matrix(1, nrow = n, ncol = c)
# now we need to reorder the columns so that they are in the order of most correlated
cor.mat.ord <- cor.mat[,order(-abs(cor.mat[1,]))]
cor.mat.ord <- cor.mat.ord[order(-abs(cor.mat[,1])),]
# loop through the data and remove the data that is too correlated to one another
for (i in 2:c) {
if (i == 2) {
m[,i] <- m[,i]
}
if (i > 2) {
red.mat <- m[,1:(i-1)]
cor.index <- which(red.mat[1,] == 1)
var.cors <- as.numeric(abs(cor.mat.ord[i, cor.index[2:length(cor.index)]]))
ifelse(any(var.cors > R), m[,i] <- 0, m[,i] <- m[,i])
}
}
# save the column names of the variables that we kept
name.index <- which(m[1,] == 1)
cor.names <- colnames(cor.mat.ord)
final.names <- cor.names[name.index]
# pull out the original data from the above variables
og.index <- match(final.names, colnames(new))
out <- new[,og.index]
return(data.frame(out))
}
# set the R2 cutoff point for the above function
# try changing this to make sure we are avoiding multicollinearity
R <- 0.5
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
# run an OLS with the above predictors
npp.model <- lm(npp ~ .,
data = reduced.all.data)
summary(npp.model)
# run a backwards stepwise regression to reduce predictors
aic.model <- stepAIC(npp.model, k = 2, trace = F)
summary(aic.model)
# remove the non-significant predictors p > 0.001
npp.pred.names <- names(which(summary(aic.model)$coefficients[,4][2:length(summary(aic.model)$coefficients)] < 0.001))
# find the index of these predictors
pred.index <- which(colnames(reduced.all.data) %in% npp.pred.names)
# use the index to pull out the final data
final.pred <- reduced.all.data[,c(1,pred.index)]
# run the final regression
model.final <- lm(npp ~ .,
data = final.pred)
summary(model.final)
# calculate geographical trend variables using the scaled data
x.sc <- standard(locs.short[,1])
y.sc <- standard(locs.short[,2])
x.sq <- x.sc^2
x.y <- x.sc * y.sc
y.sq <- y.sc^2
x.cub <- x.sc^3
y.cub <- y.sc^3
x2.y <- x.sq * y.sc
x.y2 <- x.sc * y.sq
geographic <- cbind(x.sc, y.sc, x.sq, x.y, y.sq, x.cub, y.cub, x2.y, x.y2)
# now we scale those geographic trends again
geog.sc <- as.data.frame(apply(geographic, 2, standard))
# recombine dataset, with the scaled original data, the scaled geographic data, and the
# original binary variables
all.data <- cbind(scaled.in.data.short, geog.sc, in.data.sub[,44:48])
head(all.data)
# write up a function to use to remove correlated data
rem.cors <- function(frame) {
# find the number of columns and rows in the dataframe
b <- ncol(frame)
n <- nrow(frame)
# create an empty dataframe
keep1 <- array(0, dim = c(1, b))
# remove columns that are either all zeros or all the same value
for (h in 1:b) {
ifelse(sum(frame[,h]) != 0, keep1[,h] <- h, keep1[,h] <- 0)
ifelse(sum(duplicated(frame[,h])) != n - 1 ,keep1[,h] <- h, keep1[,h] <- 0)
}
# only keep the columns that have values
g <- as.numeric(keep1[keep1 > 0])
new <- frame[,g]
# find the number of columns left after removing no data values
c <- ncol(new)
# create a correlation matrix (will be c x c dimensions)
cor.mat <- cor(new, method = "pearson", use = "na.or.complete")
# make an empty matrix filled with zeros
keep <- array(0, dim = c(1,c))
# make an empty matrix filled with ones
m <- matrix(1, nrow = n, ncol = c)
# now we need to reorder the columns so that they are in the order of most correlated
cor.mat.ord <- cor.mat[,order(-abs(cor.mat[1,]))]
cor.mat.ord <- cor.mat.ord[order(-abs(cor.mat[,1])),]
# loop through the data and remove the data that is too correlated to one another
for (i in 2:c) {
if (i == 2) {
m[,i] <- m[,i]
}
if (i > 2) {
red.mat <- m[,1:(i-1)]
cor.index <- which(red.mat[1,] == 1)
var.cors <- as.numeric(abs(cor.mat.ord[i, cor.index[2:length(cor.index)]]))
ifelse(any(var.cors > R), m[,i] <- 0, m[,i] <- m[,i])
}
}
# save the column names of the variables that we kept
name.index <- which(m[1,] == 1)
cor.names <- colnames(cor.mat.ord)
final.names <- cor.names[name.index]
# pull out the original data from the above variables
og.index <- match(final.names, colnames(new))
out <- new[,og.index]
return(data.frame(out))
}
# set the R2 cutoff point for the above function
# try changing this to make sure we are avoiding multicollinearity
R <- 0.5
# remove multicollinear data
reduced.all.data <- rem.cors(all.data)
# check the data
dim(reduced.all.data)
colnames(reduced.all.data)
# run an OLS with the above predictors
npp.model <- lm(npp ~ .,
data = reduced.all.data)
summary(npp.model)
# run a backwards stepwise regression to reduce predictors
aic.model <- stepAIC(npp.model, k = 2, trace = F)
summary(aic.model)
# remove the non-significant predictors p > 0.001
npp.pred.names <- names(which(summary(aic.model)$coefficients[,4][2:length(summary(aic.model)$coefficients)] < 0.001))
# find the index of these predictors
pred.index <- which(colnames(reduced.all.data) %in% npp.pred.names)
# use the index to pull out the final data
final.pred <- reduced.all.data[,c(1,pred.index)]
# run the final regression
model.final <- lm(npp ~ .,
data = final.pred)
summary(model.final)
library(devtools)
setwd("C:/Users/Aaron Kamoske/Dropbox/R_Packages_GitHub/hypRspec")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(devtools)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
