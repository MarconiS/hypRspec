test.n <- round(nrow(chem.spec.data)*0.2)
test.data <- sample_n(chem.spec.data, test.n,
replace = FALSE,
weight = chem.spec.data$N)
# then the rest of the chem.spec.data are training data
training.data <- chem.spec.data[!(chem.spec.data$ID %in% test.data$ID),]
# lets subset out the spectra data so we can just use that
# this is getting ready to format the data to work with the PLRS function
spectra.training.data <- training.data[,3:ncol(training.data)]
n.training.data <- training.data[,1:2]
spectra.test.data <- test.data[,3:ncol(test.data)]
n.test.data <- test.data[,1:2]
#first lets set some options inside the pls package
pls.options(plsralg = "oscorespls")
pls.options("plsralg")
#lets save the name of the N variable we are using so that if it changes from
# csv to csv we don't have to change the code
in.var <- "leaf.N"
#lets put our dataset into the correct format for running the PLSR
plsr.spectra <- as.matrix(spectra.training.data)
plsr.dataset <- data.frame(leaf.N = n.training.data$N,
spectra = I(as.matrix(spectra.training.data)))
# lets take a quick look at the correlations between the spectra and biochemical
# data (note from Kyla - this seems like a clunky way to do this, but it works)
spectra.cor <- cor(plsr.spectra,
plsr.dataset[grep(in.var, names(plsr.dataset), fixed = TRUE)],
use = "complete.obs")
#lets take a look at the spectra again (narrower range)
matplot(t(plsr.spectra),
type = "l",
xlab = "Wavelength (nm)",
ylab = "HSI Reflectance",
main = "2018 All Sites HSI spectra - training data")
#lets take a look at the correlation between the spectra and biochemical data
plot(spectra.cor,
xlab = "Wavelength (nm)",
ylab = "Correlation",
type = "l",
lwd = 4)
abline(h = 0,lty = 2, lwd = 1.5, col = "grey80")
box(lwd = 2)
#lets write this data to a csv so we can reference it later on
spectra.cor.df <- data.frame(spectra.cor)
names(spectra.cor.df) <- c("Correlation")
write.csv(spectra.cor.df, paste0(out.dir, in.var, '_Spectra_Correlations.csv',
sep = ""),
row.names = TRUE)
#first lets find the dimensions of our dataset and set some parameters
dims <- dim(plsr.dataset)
n.comps <- 15
iterations <- 50
prop <- 0.80
#lets create an empty matrix to store our results in
jk.out <- matrix(data = NA, nrow = iterations, ncol = n.comps)
#lets start a timer to see how long this takes to run
start.time <- Sys.time()
#lets run through all the different iterations of this
for (i in 1:iterations) {
#remind ourselves what iteration we are on
print(paste("Iteration: ", i, sep = ""))
#lets take a sample from our dataset to test this on
rows <- sample(1:nrow(plsr.dataset), floor(prop*nrow(plsr.dataset)))
sub.data <- plsr.dataset[rows,]
#lets run our PLSR model now
plsr.out <- plsr(as.formula(paste(in.var,"~","spectra")), scale = FALSE,
ncomp = n.comps, validation = "LOO",
trace = TRUE, data = sub.data)
#lets save our press statistic in our empty matrix
resPRESS <- as.vector(plsr.out$validation$PRESS)
jk.out[i,seq(plsr.out$validation$ncomp)] = resPRESS
}
#lets find out how long it took the jackknife to run
end.time <- Sys.time()
end.time - start.time
#lets change our output matrix to a dataframe for easier manipulation
pressDF <- as.data.frame(jk.out)
#lets name the columns
names(pressDF) <- as.character(seq(n.comps))
#lets write this as a csv for later use
write.csv(pressDF, file = paste0(out.dir, in.var,
"_Jackkife_PLSR_Coefficients.csv"),
row.names = FALSE)
#lets melt the data for easier plotting
pressDFres <- melt(pressDF)
#lets see what our press statistics look like. small is better for this.
boxplot(pressDFres$value ~ pressDFres$variable,
xlab = "n Components",
ylab = "PRESS",
main = in.var)
# How many components? Can use this to determine if next largest is sig different
# than lower.  Then lower is best.
# We can do this with a simple T-Test - a smaller PRESS statistic is better. so
# lets see where this starts to vary. we want the lowest number of components so
# that we don't over predict our model.
loc.1 <- 3
loc.2 <- 4
ttest <- t.test(pressDFres$value[which(pressDFres$variable == loc.1)],
pressDFres$value[which(pressDFres$variable == loc.2)])
#By examining the out put we can determine what the best number of components are to avoid overfitting.
ttest
# How many components? Can use this to determine if next largest is sig different
# than lower.  Then lower is best.
# We can do this with a simple T-Test - a smaller PRESS statistic is better. so
# lets see where this starts to vary. we want the lowest number of components so
# that we don't over predict our model.
loc.1 <- 4
loc.2 <- 5
ttest <- t.test(pressDFres$value[which(pressDFres$variable == loc.1)],
pressDFres$value[which(pressDFres$variable == loc.2)])
#By examining the out put we can determine what the best number of components are to avoid overfitting.
ttest
#since we see a low p-value we can see that there is no difference between the
# two variables now. so lets go with the smaller value.
# Now that we know the number of test components lets run our PLSR model again
# with that number of components.
nComps <- 4
plsr.out <- plsr(as.formula(paste(in.var, "~", "spectra")), scale = FALSE,
ncomp = n.comps, validation = "LOO",
trace = TRUE, data = plsr.dataset)
#lets save our fitted values
fit1 <- plsr.out$fitted.values[, 1, nComps]
#lets plot them to see what they look like
plot(c(0,4), c(0,4),
xlab = "PLSR Fitted Values %N",
ylab = "Training Data %N")
points(fit1, plsr.dataset[,in.var])
abline(lm(plsr.dataset[,in.var] ~ fit1), lwd = 2)
abline(0, 1, col = "red", lwd = 2, lty = 2)
#lets plot them to see what they look like
plot(c(0,4), c(0,4),
xlab = "PLSR Fitted Values %N",
ylab = "Training Data %N")
points(fit1, plsr.dataset[,in.var])
abline(lm(plsr.dataset[,in.var] ~ fit1), lwd = 2)
abline(0, 1, col = "red", lwd = 2, lty = 2)
summary(lm(plsr.dataset[,in.var] ~ fit1))
# note, this is a place to change depending on results
# might want to try a narrower or wider range or subset of spectra
# we can find these index values by doing this
which(colnames(all.data) =="nm737.10553")
# note, this is a place to change depending on results
# might want to try a narrower or wider range or subset of spectra
# we can find these index values by doing this
which(colnames(all.data) =="nm1348.359619")
# note, this is a place to change depending on results
# might want to try a narrower or wider range or subset of spectra
# we can find these index values by doing this
which(colnames(all.data) =="nm1648.976318")
# note, this is a place to change depending on results
# might want to try a narrower or wider range or subset of spectra
# we can find these index values by doing this
which(colnames(all.data) =="nm1789.26416")
keep.data <- all.data[,c(1, 54, 7:176, 216:244)]
# also get the spectra values in the same range (this will be useful for plotting later)
keep.spectra <- as.data.frame(names(all.data[, c(7:176, 216:244)]))
# keep the data that have leaf chem and spectra
chem.spec.data <- subset(keep.data, !is.na(keep.data$N))
# lets take ~20% of our data for testing and leave the other 80% for training
# this should sample more or less evenly across the distribution of N values
test.n <- round(nrow(chem.spec.data)*0.2)
test.data <- sample_n(chem.spec.data, test.n,
replace = FALSE,
weight = chem.spec.data$N)
# then the rest of the chem.spec.data are training data
training.data <- chem.spec.data[!(chem.spec.data$ID %in% test.data$ID),]
# lets subset out the spectra data so we can just use that
# this is getting ready to format the data to work with the PLRS function
spectra.training.data <- training.data[,3:ncol(training.data)]
n.training.data <- training.data[,1:2]
spectra.test.data <- test.data[,3:ncol(test.data)]
n.test.data <- test.data[,1:2]
#first lets set some options inside the pls package
pls.options(plsralg = "oscorespls")
pls.options("plsralg")
#lets save the name of the N variable we are using so that if it changes from
# csv to csv we don't have to change the code
in.var <- "leaf.N"
#lets put our dataset into the correct format for running the PLSR
plsr.spectra <- as.matrix(spectra.training.data)
plsr.dataset <- data.frame(leaf.N = n.training.data$N,
spectra = I(as.matrix(spectra.training.data)))
# lets take a quick look at the correlations between the spectra and biochemical
# data (note from Kyla - this seems like a clunky way to do this, but it works)
spectra.cor <- cor(plsr.spectra,
plsr.dataset[grep(in.var, names(plsr.dataset), fixed = TRUE)],
use = "complete.obs")
#lets take a look at the spectra again (narrower range)
matplot(t(plsr.spectra),
type = "l",
xlab = "Wavelength (nm)",
ylab = "HSI Reflectance",
main = "2018 All Sites HSI spectra - training data")
#lets take a look at the correlation between the spectra and biochemical data
plot(spectra.cor,
xlab = "Wavelength (nm)",
ylab = "Correlation",
type = "l",
lwd = 4)
keep.data
keep.data <- all.data[,c(1, 5, 54:176, 216:244)]
# also get the spectra values in the same range (this will be useful for plotting later)
keep.spectra <- as.data.frame(names(all.data[, c(7:176, 216:244)]))
# keep the data that have leaf chem and spectra
chem.spec.data <- subset(keep.data, !is.na(keep.data$N))
# lets take ~20% of our data for testing and leave the other 80% for training
# this should sample more or less evenly across the distribution of N values
test.n <- round(nrow(chem.spec.data)*0.2)
test.data <- sample_n(chem.spec.data, test.n,
replace = FALSE,
weight = chem.spec.data$N)
# then the rest of the chem.spec.data are training data
training.data <- chem.spec.data[!(chem.spec.data$ID %in% test.data$ID),]
# lets subset out the spectra data so we can just use that
# this is getting ready to format the data to work with the PLRS function
spectra.training.data <- training.data[,3:ncol(training.data)]
n.training.data <- training.data[,1:2]
spectra.test.data <- test.data[,3:ncol(test.data)]
n.test.data <- test.data[,1:2]
#first lets set some options inside the pls package
pls.options(plsralg = "oscorespls")
pls.options("plsralg")
#lets save the name of the N variable we are using so that if it changes from
# csv to csv we don't have to change the code
in.var <- "leaf.N"
#lets put our dataset into the correct format for running the PLSR
plsr.spectra <- as.matrix(spectra.training.data)
plsr.dataset <- data.frame(leaf.N = n.training.data$N,
spectra = I(as.matrix(spectra.training.data)))
# lets take a quick look at the correlations between the spectra and biochemical
# data (note from Kyla - this seems like a clunky way to do this, but it works)
spectra.cor <- cor(plsr.spectra,
plsr.dataset[grep(in.var, names(plsr.dataset), fixed = TRUE)],
use = "complete.obs")
#lets take a look at the spectra again (narrower range)
matplot(t(plsr.spectra),
type = "l",
xlab = "Wavelength (nm)",
ylab = "HSI Reflectance",
main = "2018 All Sites HSI spectra - training data")
#lets take a look at the correlation between the spectra and biochemical data
plot(spectra.cor,
xlab = "Wavelength (nm)",
ylab = "Correlation",
type = "l",
lwd = 4)
abline(h = 0,lty = 2, lwd = 1.5, col = "grey80")
box(lwd = 2)
#lets write this data to a csv so we can reference it later on
spectra.cor.df <- data.frame(spectra.cor)
names(spectra.cor.df) <- c("Correlation")
write.csv(spectra.cor.df, paste0(out.dir, in.var, '_Spectra_Correlations.csv',
sep = ""),
row.names = TRUE)
#lets write this data to a csv so we can reference it later on
spectra.cor.df <- data.frame(spectra.cor)
names(spectra.cor.df) <- c("Correlation")
write.csv(spectra.cor.df, paste0(out.dir, in.var, '_Spectra_Correlations.csv',
sep = ""),
row.names = TRUE)
#first lets find the dimensions of our dataset and set some parameters
dims <- dim(plsr.dataset)
n.comps <- 15
iterations <- 50
prop <- 0.80
#lets create an empty matrix to store our results in
jk.out <- matrix(data = NA, nrow = iterations, ncol = n.comps)
#lets start a timer to see how long this takes to run
start.time <- Sys.time()
#lets run through all the different iterations of this
for (i in 1:iterations) {
#remind ourselves what iteration we are on
print(paste("Iteration: ", i, sep = ""))
#lets take a sample from our dataset to test this on
rows <- sample(1:nrow(plsr.dataset), floor(prop*nrow(plsr.dataset)))
sub.data <- plsr.dataset[rows,]
#lets run our PLSR model now
plsr.out <- plsr(as.formula(paste(in.var,"~","spectra")), scale = FALSE,
ncomp = n.comps, validation = "LOO",
trace = TRUE, data = sub.data)
#lets save our press statistic in our empty matrix
resPRESS <- as.vector(plsr.out$validation$PRESS)
jk.out[i,seq(plsr.out$validation$ncomp)] = resPRESS
}
#lets find out how long it took the jackknife to run
end.time <- Sys.time()
end.time - start.time
#lets change our output matrix to a dataframe for easier manipulation
pressDF <- as.data.frame(jk.out)
#lets name the columns
names(pressDF) <- as.character(seq(n.comps))
#lets write this as a csv for later use
write.csv(pressDF, file = paste0(out.dir, in.var,
"_Jackkife_PLSR_Coefficients.csv"),
row.names = FALSE)
#lets melt the data for easier plotting
pressDFres <- melt(pressDF)
#lets see what our press statistics look like. small is better for this.
boxplot(pressDFres$value ~ pressDFres$variable,
xlab = "n Components",
ylab = "PRESS",
main = in.var)
# How many components? Can use this to determine if next largest is sig different
# than lower.  Then lower is best.
# We can do this with a simple T-Test - a smaller PRESS statistic is better. so
# lets see where this starts to vary. we want the lowest number of components so
# that we don't over predict our model.
loc.1 <- 4
loc.2 <- 5
ttest <- t.test(pressDFres$value[which(pressDFres$variable == loc.1)],
pressDFres$value[which(pressDFres$variable == loc.2)])
#By examining the out put we can determine what the best number of components are to avoid overfitting.
ttest
# How many components? Can use this to determine if next largest is sig different
# than lower.  Then lower is best.
# We can do this with a simple T-Test - a smaller PRESS statistic is better. so
# lets see where this starts to vary. we want the lowest number of components so
# that we don't over predict our model.
loc.1 <- 3
loc.2 <- 4
ttest <- t.test(pressDFres$value[which(pressDFres$variable == loc.1)],
pressDFres$value[which(pressDFres$variable == loc.2)])
#By examining the out put we can determine what the best number of components are to avoid overfitting.
ttest
# How many components? Can use this to determine if next largest is sig different
# than lower.  Then lower is best.
# We can do this with a simple T-Test - a smaller PRESS statistic is better. so
# lets see where this starts to vary. we want the lowest number of components so
# that we don't over predict our model.
loc.1 <- 2
loc.2 <- 3
ttest <- t.test(pressDFres$value[which(pressDFres$variable == loc.1)],
pressDFres$value[which(pressDFres$variable == loc.2)])
#By examining the out put we can determine what the best number of components are to avoid overfitting.
ttest
#since we see a low p-value we can see that there is no difference between the
# two variables now. so lets go with the smaller value.
# Now that we know the number of test components lets run our PLSR model again
# with that number of components.
nComps <- 2
plsr.out <- plsr(as.formula(paste(in.var, "~", "spectra")), scale = FALSE,
ncomp = n.comps, validation = "LOO",
trace = TRUE, data = plsr.dataset)
#lets save our fitted values
fit1 <- plsr.out$fitted.values[, 1, nComps]
#lets plot them to see what they look like
plot(c(0,4), c(0,4),
xlab = "PLSR Fitted Values %N",
ylab = "Training Data %N")
points(fit1, plsr.dataset[,in.var])
abline(lm(plsr.dataset[,in.var] ~ fit1), lwd = 2)
abline(0, 1, col = "red", lwd = 2, lty = 2)
summary(lm(plsr.dataset[,in.var] ~ fit1))
# Install canopyLazR from GitHub
install_github("akamoske/hypRspec")
library(devtools)
# Install canopyLazR from GitHub
install_github("akamoske/hypRspec")
# Load the library
library(hypRspec)
# Calculate the NDVI mask
ndvi <- ndvi.mask(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength",
red.nm = 674,
nir.nm = 830,
ndvi.threshold = 0.5)
# Calculate the brightness mask
brightness <- brightness.mask(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength")
# Apply the topographic correction
hsi.refl <- hsi.extract(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
ndvi.mask = ndvi,
brightness.mask = brightness,
band.combo = c(25:194, 215:284, 325:403),
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength",
solar.az.path = "/TALL/Reflectance/Metadata/Logs/Solar_Azimuth_Angle",
solar.zn.path = "/TALL/Reflectance/Metadata/Logs/Solar_Zenith_Angle",
slope.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Slope",
aspect.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Aspect",
sensor.az.path = "/TALL/Reflectance/Metadata/to-sensor_Azimuth_Angle",
sensor.zn.path = "/TALL/Reflectance/Metadata/to-sensor_Zenith_Angle",
coordinate.path = "/TALL/Reflectance/Metadata/Coordinate_System",
ross = "thick",
li = "dense",
shp.file.loc = "C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/SHP_FILES/FOLIAR_DATA/TALL2018",
shp.file.name = "TALL2018_TOC_FoliarData_20190626")
library(devtools)
library(roxygen2)
devtools::document()
setwd("C:/Users/Aaron Kamoske/Dropbox/R_Packages_GitHub/hypRspec")
devtools::document()
devtools::document()
# Install canopyLazR from GitHub
install_github("akamoske/hypRspec")
# Apply the topographic correction
hsi.refl <- hsi.extract(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
ndvi.mask = ndvi,
brightness.mask = brightness,
band.combo = c(25:194, 215:284, 325:403),
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength",
solar.az.path = "/TALL/Reflectance/Metadata/Logs/Solar_Azimuth_Angle",
solar.zn.path = "/TALL/Reflectance/Metadata/Logs/Solar_Zenith_Angle",
slope.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Slope",
aspect.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Aspect",
sensor.az.path = "/TALL/Reflectance/Metadata/to-sensor_Azimuth_Angle",
sensor.zn.path = "/TALL/Reflectance/Metadata/to-sensor_Zenith_Angle",
coordinate.path = "/TALL/Reflectance/Metadata/Coordinate_System",
ross = "thick",
li = "dense",
shp.file.loc = "C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/SHP_FILES/FOLIAR_DATA/TALL2018",
shp.file.name = "TALL2018_TOC_FoliarData_20190626")
devtools::document()
# Install canopyLazR from GitHub
install_github("akamoske/hypRspec")
# Apply the topographic correction
hsi.raster <- hsi.correction(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
ndvi.mask = ndvi,
brightness.mask = brightness,
band.combo = c(25:194, 215:284, 325:403),
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength",
solar.az.path = "/TALL/Reflectance/Metadata/Logs/Solar_Azimuth_Angle",
solar.zn.path = "/TALL/Reflectance/Metadata/Logs/Solar_Zenith_Angle",
slope.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Slope",
aspect.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Aspect",
sensor.az.path = "/TALL/Reflectance/Metadata/to-sensor_Azimuth_Angle",
sensor.zn.path = "/TALL/Reflectance/Metadata/to-sensor_Zenith_Angle",
coordinate.path = "/TALL/Reflectance/Metadata/Coordinate_System",
ross = "thick",
li = "dense",
raster.res = 10)
head(hsi.refl)
head(hsi.raster)
# Apply the topographic correction
hsi.refl <- hsi.extract(hy.file = "D:/Tests/BRDF_TESTING/TALL_HDF5/NEON_D08_TALL_DP1_20180429_190316_reflectance.h5",
ndvi.mask = ndvi,
brightness.mask = brightness,
band.combo = c(25:194, 215:284, 325:403),
metadata.path = "/TALL/Reflectance/Reflectance_Data",
reflectance.path = "/TALL/Reflectance/Reflectance_Data",
wavelength.path = "/TALL/Reflectance/Metadata/Spectral_Data/Wavelength",
solar.az.path = "/TALL/Reflectance/Metadata/Logs/Solar_Azimuth_Angle",
solar.zn.path = "/TALL/Reflectance/Metadata/Logs/Solar_Zenith_Angle",
slope.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Slope",
aspect.path = "/TALL/Reflectance/Metadata/Ancillary_Imagery/Aspect",
sensor.az.path = "/TALL/Reflectance/Metadata/to-sensor_Azimuth_Angle",
sensor.zn.path = "/TALL/Reflectance/Metadata/to-sensor_Zenith_Angle",
coordinate.path = "/TALL/Reflectance/Metadata/Coordinate_System",
ross = "thick",
li = "dense",
shp.file.loc = "C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/SHP_FILES/FOLIAR_DATA/TALL2018",
shp.file.name = "TALL2018_TOC_FoliarData_20190626")
# write this to a csv file
write.csv(x = hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.tif"),
row.names = FALSE)
# write this to a csv file
write.csv(x = hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.csv"),
row.names = FALSE)
# write this to a csv file
write.csv(x = hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.csv"),
row.names = FALSE,
col.names = FALSE)
?write.csv
write.csv(x = hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.csv"),
row.names = FALSE,
col.names = FALSE)
# write this to a csv file
write.table(hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.csv"),
sep = ",",
col.names = FALSE,
row.names = FALSE)
head(hsi.refl)
# plot point 1
plot(t(hsi.refl[, 3:321]),
type = "l",
ylim = c(0,1),
ylab = "POINT 1",
col = "blue",
axes = FALSE,
frame.plot = TRUE,
main = "RAW")
# write this to a csv file
write.table(hsi.refl,
file = paste0("C:/Users/Aaron Kamoske/Dropbox/Dissertation/Data/PROCESSED_DATA/HSI_REFLECTANCE/TALL2018/Flightlines/TALL2018_",
"testing",
"_TOC_Reflectance_20190701.csv"),
sep = ",",
col.names = FALSE,
row.names = FALSE)
